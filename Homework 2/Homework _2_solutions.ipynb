{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYwLs5joJfJN"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\"><b></b>\n",
        "<h1><center> <font color='black'> Homework 02  </font></center></h1>\n",
        "<h2><center> <font color='black'> Regression & Regularization</font></center></h2>    \n",
        "<h2><center> <font color='black'> MTAT.03.319 - Business Data Analytics</font></center></h2>\n",
        "<h2><center> <font color='black'> University of Tartu - Spring 2021</font></center></h2>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMAvgHx6JfJm"
      },
      "source": [
        "# Homework instructions\n",
        "\n",
        "- Please provide the names and student IDs of the team-members (Maximum 2 person) in the field \"Team mates\" below. If you are not working in a team please insert only your name and student ID. \n",
        "\n",
        "- The accepted submission formats are Colab links or .ipynb files. If you are submitting Colab links please make sure that the privacy settings for the file is public so we can access your code. \n",
        "\n",
        "- The submission will automatically close on <font color='red'>**21 March at 23:59**</font>, so please make sure to submit before the deadline. \n",
        "\n",
        "- ONLY one of the teammates should submit the homework. We will grade the homework and the marks and feedback is applied for both the team members. So please communicate with your team member about marks and feedback if you are submit the homework.\n",
        "\n",
        "- If a question is not clear, please ask us in Moodle ONLY. \n",
        "\n",
        "- After you have finished solving the Homework, please restart the Kernel and run all the cells to check if there is any persisting issues. \n",
        "\n",
        "- Plagiarism is <font color='red'>**PROHIBITED**</font>. Any form of plagiarism will be dealt according to the university policy (https://www.ut.ee/en/current-students/academic-fraud).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQFbVbtpJfJn"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "* In this homework you are going to apply supervised learning: Linear Regression method using Scikit-learn package; Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy [https://en.wikipedia.org/wiki/Scikit-learn].\n",
        "\n",
        "### The homework is divided into four sections and the points are distributed as below:\n",
        "<pre>\n",
        "- Linear Regression    -> 2 points\n",
        "- PCA                  -> 2 points\n",
        "- Overfitting          -> 5 points\n",
        "_________________________________________\n",
        "Total                  -> 9 points\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1wszlIpJfJo"
      },
      "source": [
        "# 1. Regression \n",
        "## 1.1 Linear Regression (2 points)\n",
        "\n",
        "We are going to use the Prices dataset that contains 74 columns. Each column represents a feature of houses for sale. The ```SalePrice``` column  shows their prices. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r7nLZI_sijVH"
      },
      "outputs": [],
      "source": [
        "# import all libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler # preprocessing\n",
        "from sklearn.preprocessing import PolynomialFeatures # preprocessing\n",
        "from sklearn.preprocessing import scale # preprocessing\n",
        "from sklearn.feature_selection import RFE # preprocessing\n",
        "\n",
        "from sklearn.linear_model import LinearRegression # ML alg\n",
        "\n",
        "# cross validation \n",
        "from sklearn.model_selection import cross_val_score \n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# import warnings # supress warnings\n",
        "# warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qidamLnMJfJo",
        "outputId": "42a793b0-97b2-499a-94da-39332d201eed",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ac81ae94-5d1f-46b1-b283-8087a6fa132d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>LandSlope</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>Condition1</th>\n",
              "      <th>Condition2</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>RoofStyle</th>\n",
              "      <th>RoofMatl</th>\n",
              "      <th>Exterior1st</th>\n",
              "      <th>Exterior2nd</th>\n",
              "      <th>MasVnrType</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>ExterQual</th>\n",
              "      <th>ExterCond</th>\n",
              "      <th>Foundation</th>\n",
              "      <th>BsmtQual</th>\n",
              "      <th>BsmtCond</th>\n",
              "      <th>BsmtExposure</th>\n",
              "      <th>BsmtFinType1</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinType2</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>Heating</th>\n",
              "      <th>HeatingQC</th>\n",
              "      <th>CentralAir</th>\n",
              "      <th>Electrical</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>KitchenQual</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Functional</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageType</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageFinish</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageQual</th>\n",
              "      <th>GarageCond</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>196.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>No</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>706</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>150</td>\n",
              "      <td>856</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>8</td>\n",
              "      <td>Typ</td>\n",
              "      <td>0</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2003.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>548</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>61</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>Reg</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>Feedr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>MetalSd</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>CBlock</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>978</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>284</td>\n",
              "      <td>1262</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>TA</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>1976.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>460</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>298</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Inside</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>2002</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>162.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Mn</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>486</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>434</td>\n",
              "      <td>920</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>6</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2001.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>2</td>\n",
              "      <td>608</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>Corner</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1970</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>Wd Sdng</td>\n",
              "      <td>Wd Shng</td>\n",
              "      <td>None</td>\n",
              "      <td>0.0</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>BrkTil</td>\n",
              "      <td>TA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>No</td>\n",
              "      <td>ALQ</td>\n",
              "      <td>216</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>540</td>\n",
              "      <td>756</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Gd</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>7</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Detchd</td>\n",
              "      <td>1998.0</td>\n",
              "      <td>Unf</td>\n",
              "      <td>3</td>\n",
              "      <td>642</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>272</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>WD</td>\n",
              "      <td>Abnorml</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>Lvl</td>\n",
              "      <td>AllPub</td>\n",
              "      <td>FR2</td>\n",
              "      <td>Gtl</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>Norm</td>\n",
              "      <td>Norm</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>Gable</td>\n",
              "      <td>CompShg</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>VinylSd</td>\n",
              "      <td>BrkFace</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>PConc</td>\n",
              "      <td>Gd</td>\n",
              "      <td>TA</td>\n",
              "      <td>Av</td>\n",
              "      <td>GLQ</td>\n",
              "      <td>655</td>\n",
              "      <td>Unf</td>\n",
              "      <td>0</td>\n",
              "      <td>490</td>\n",
              "      <td>1145</td>\n",
              "      <td>GasA</td>\n",
              "      <td>Ex</td>\n",
              "      <td>Y</td>\n",
              "      <td>SBrkr</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Gd</td>\n",
              "      <td>9</td>\n",
              "      <td>Typ</td>\n",
              "      <td>1</td>\n",
              "      <td>Attchd</td>\n",
              "      <td>2000.0</td>\n",
              "      <td>RFn</td>\n",
              "      <td>3</td>\n",
              "      <td>836</td>\n",
              "      <td>TA</td>\n",
              "      <td>TA</td>\n",
              "      <td>Y</td>\n",
              "      <td>192</td>\n",
              "      <td>84</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>WD</td>\n",
              "      <td>Normal</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac81ae94-5d1f-46b1-b283-8087a6fa132d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac81ae94-5d1f-46b1-b283-8087a6fa132d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac81ae94-5d1f-46b1-b283-8087a6fa132d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   MSSubClass MSZoning  LotFrontage  ...  SaleType SaleCondition SalePrice\n",
              "0          60       RL         65.0  ...        WD        Normal    208500\n",
              "1          20       RL         80.0  ...        WD        Normal    181500\n",
              "2          60       RL         68.0  ...        WD        Normal    223500\n",
              "3          70       RL         60.0  ...        WD       Abnorml    140000\n",
              "4          60       RL         84.0  ...        WD        Normal    250000\n",
              "\n",
              "[5 rows x 75 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"Prices.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEnDac4UijVK",
        "outputId": "05bcdf1a-c678-4f3a-987e-db69a21b1057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Electrical        1\n",
              "MasVnrType        8\n",
              "MasVnrArea        8\n",
              "BsmtQual         37\n",
              "BsmtCond         37\n",
              "BsmtFinType1     37\n",
              "BsmtFinType2     38\n",
              "BsmtExposure     38\n",
              "GarageFinish     81\n",
              "GarageQual       81\n",
              "GarageCond       81\n",
              "GarageYrBlt      81\n",
              "GarageType       81\n",
              "LotFrontage     259\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#All NaN values of the dataset\n",
        "data.isna().sum().sort_values().tail(14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5dxc_JaJfJq"
      },
      "source": [
        "The column names are self-explanatory which indicates features of each house."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt1Kln3rJfJq"
      },
      "source": [
        "**1.1.1. The target label is```SalePrice``` which means, later we will predict the sale-price based on the given features (columns). But for regression task, it is important to ensure that the data is not skewed. In order to do that, please plot the distribution of ```SalePrice``` column and explain what do you see. (0.2 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IJbRJKE8JfJq",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9744e9a5-93ce-4fc0-a2a1-94582f4cf21a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(data[\"SalePrice\"], bins=50)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UthxdZLyJfJq"
      },
      "source": [
        "**<font color='red'>Answer:</font>** According to the plot, data is right skewed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCEjKtlJJfJr"
      },
      "source": [
        "So, the data seems to be skewed which has to be fixed otherwise it may lead to erronous result. \n",
        "Apart from that, look closely, some columns are not numerical. For those, you have to convert them to numerical value or represent them in a way so that the algorithm can understand the data. One of such way is called, one hot encoding. Along with that, the algorithm cannot deal with NaN or Infinite values. So please address all of these in the preprocessing section. \n",
        "\n",
        "- Preprocess for skewed data\n",
        "- Apply one-hot encoding to categorical data types\n",
        "- Replace negative NaN and infinite values with 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1s3u1LLJfJr"
      },
      "source": [
        "**1.1.2. After preprocessing the skewed data, plot ```SalePrice``` column distribution again. (0.05 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hl0qQwwdJfJs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "efd9d0c8-cb6c-4fae-f026-3eacb1f3c885"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ddf94535-7ef2-459d-97d0-58f7cecdb653\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>MasVnrArea</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>BsmtFinSF2</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>LowQualFinSF</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>KitchenAbvGr</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageYrBlt</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>EnclosedPorch</th>\n",
              "      <th>3SsnPorch</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SalePrice</th>\n",
              "      <th>MSZoning_C (all)</th>\n",
              "      <th>MSZoning_FV</th>\n",
              "      <th>MSZoning_RH</th>\n",
              "      <th>...</th>\n",
              "      <th>Functional_Mod</th>\n",
              "      <th>Functional_Sev</th>\n",
              "      <th>Functional_Typ</th>\n",
              "      <th>GarageType_2Types</th>\n",
              "      <th>GarageType_Attchd</th>\n",
              "      <th>GarageType_Basment</th>\n",
              "      <th>GarageType_BuiltIn</th>\n",
              "      <th>GarageType_CarPort</th>\n",
              "      <th>GarageType_Detchd</th>\n",
              "      <th>GarageFinish_Fin</th>\n",
              "      <th>GarageFinish_RFn</th>\n",
              "      <th>GarageFinish_Unf</th>\n",
              "      <th>GarageQual_Ex</th>\n",
              "      <th>GarageQual_Fa</th>\n",
              "      <th>GarageQual_Gd</th>\n",
              "      <th>GarageQual_Po</th>\n",
              "      <th>GarageQual_TA</th>\n",
              "      <th>GarageCond_Ex</th>\n",
              "      <th>GarageCond_Fa</th>\n",
              "      <th>GarageCond_Gd</th>\n",
              "      <th>GarageCond_Po</th>\n",
              "      <th>GarageCond_TA</th>\n",
              "      <th>PavedDrive_N</th>\n",
              "      <th>PavedDrive_P</th>\n",
              "      <th>PavedDrive_Y</th>\n",
              "      <th>SaleType_COD</th>\n",
              "      <th>SaleType_CWD</th>\n",
              "      <th>SaleType_Con</th>\n",
              "      <th>SaleType_ConLD</th>\n",
              "      <th>SaleType_ConLI</th>\n",
              "      <th>SaleType_ConLw</th>\n",
              "      <th>SaleType_New</th>\n",
              "      <th>SaleType_Oth</th>\n",
              "      <th>SaleType_WD</th>\n",
              "      <th>SaleCondition_Abnorml</th>\n",
              "      <th>SaleCondition_AdjLand</th>\n",
              "      <th>SaleCondition_Alloca</th>\n",
              "      <th>SaleCondition_Family</th>\n",
              "      <th>SaleCondition_Normal</th>\n",
              "      <th>SaleCondition_Partial</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.094345</td>\n",
              "      <td>4.174387</td>\n",
              "      <td>9.041922</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>7.602401</td>\n",
              "      <td>7.602401</td>\n",
              "      <td>5.278115</td>\n",
              "      <td>6.559615</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.010635</td>\n",
              "      <td>6.752270</td>\n",
              "      <td>6.752270</td>\n",
              "      <td>6.749931</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.444249</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.602401</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>6.306275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.110874</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>7.604894</td>\n",
              "      <td>12.247694</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.995732</td>\n",
              "      <td>4.382027</td>\n",
              "      <td>9.169518</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>7.588830</td>\n",
              "      <td>7.588830</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.885510</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.648974</td>\n",
              "      <td>7.140453</td>\n",
              "      <td>7.140453</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.140453</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.588830</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>6.131226</td>\n",
              "      <td>5.697093</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>7.604396</td>\n",
              "      <td>12.109011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.094345</td>\n",
              "      <td>4.219508</td>\n",
              "      <td>9.328123</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>7.601402</td>\n",
              "      <td>7.601902</td>\n",
              "      <td>5.087596</td>\n",
              "      <td>6.186209</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.073045</td>\n",
              "      <td>6.824374</td>\n",
              "      <td>6.824374</td>\n",
              "      <td>6.763885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.487734</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.791759</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.601402</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>6.410175</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.737670</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>7.604894</td>\n",
              "      <td>12.317167</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.248495</td>\n",
              "      <td>4.094345</td>\n",
              "      <td>9.164296</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>7.557473</td>\n",
              "      <td>7.585789</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.375278</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.291569</td>\n",
              "      <td>6.628041</td>\n",
              "      <td>6.867974</td>\n",
              "      <td>6.628041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.448334</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.945910</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.599902</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>6.464588</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.555348</td>\n",
              "      <td>5.605802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>7.603898</td>\n",
              "      <td>11.849398</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.094345</td>\n",
              "      <td>4.430817</td>\n",
              "      <td>9.565214</td>\n",
              "      <td>2.079442</td>\n",
              "      <td>1.609438</td>\n",
              "      <td>7.600902</td>\n",
              "      <td>7.600902</td>\n",
              "      <td>5.857933</td>\n",
              "      <td>6.484635</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.194405</td>\n",
              "      <td>7.043160</td>\n",
              "      <td>7.043160</td>\n",
              "      <td>6.959399</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.695303</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.693147</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.386294</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.197225</td>\n",
              "      <td>1.0</td>\n",
              "      <td>7.600902</td>\n",
              "      <td>1.098612</td>\n",
              "      <td>6.728629</td>\n",
              "      <td>5.257495</td>\n",
              "      <td>4.430817</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.484907</td>\n",
              "      <td>7.604894</td>\n",
              "      <td>12.429216</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 271 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ddf94535-7ef2-459d-97d0-58f7cecdb653')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ddf94535-7ef2-459d-97d0-58f7cecdb653 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ddf94535-7ef2-459d-97d0-58f7cecdb653');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   MSSubClass  LotFrontage  ...  SaleCondition_Normal  SaleCondition_Partial\n",
              "0    4.094345     4.174387  ...                   1.0                    0.0\n",
              "1    2.995732     4.382027  ...                   1.0                    0.0\n",
              "2    4.094345     4.219508  ...                   1.0                    0.0\n",
              "3    4.248495     4.094345  ...                   0.0                    0.0\n",
              "4    4.094345     4.430817  ...                   1.0                    0.0\n",
              "\n",
              "[5 rows x 271 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "hotdata = pd.get_dummies(data)\n",
        "newhot= hotdata.replace(-np.inf,0)\n",
        "\n",
        "newhot = np.log(newhot)\n",
        "\n",
        "newhot = newhot.replace(0, 1)\n",
        "\n",
        "newhot = newhot.replace(-np.inf, 0)\n",
        "newhot.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "Rmj0lM2WijVN",
        "outputId": "e3f2a4ec-59b4-4073-dc62-ea0d29af5ad1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.hist(newhot[\"SalePrice\"], bins=50)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQPBZ0QWJfJs"
      },
      "source": [
        "**1.1.3. Calculate the correlation between price and each feature. Which are the top 3 features that have the highest correlation with  price? Is the correlation positive or negative? Explain what happens with the price when each of those 3 features change (consider only one feature at a time) and others are kept constant. (0.25 point)** \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A59X1K5fJfJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d308334-767c-49db-fa6e-c67a2f7a1373"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1stFlrSF       0.608947\n",
            "GrLivArea      0.730255\n",
            "OverallQual    0.801120\n",
            "SalePrice      1.000000\n",
            "Name: SalePrice, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "corrdata=newhot[newhot.columns[0:]].corr()['SalePrice'][:-1]\n",
        "print(corrdata.sort_values().tail(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLYyFSd-JfJt"
      },
      "source": [
        "<font color='red'> **Answer:**</font> The top 3 features are: OverallQual, GrLivArea, 1stFlrSF\n",
        "- It seems correlations are positive\n",
        "- The price increase  0.801120, 0.730255, 0.608947 respectively (3 features shown above) when we increase 1 unit of those features.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qQzbp4RJfJt"
      },
      "source": [
        "**1.1.4.  Now you have to build a regression model that would be trained on training data and later predict the price on test data. You are free to select features on which you want train the model. The dataset has missing values, so please apply the following methods for dealing with the missing data in the features of your choice:**\n",
        "\n",
        "a) mean imputation\n",
        "\n",
        "b) median imputation\n",
        "\n",
        "c) mode imputation\n",
        "\n",
        "d) dropping missing values\n",
        "\n",
        "**Split dataset into the training (80% of the all rows) and test ( 20% of all rows) set, you can use train_test_split function from scikit-learn. While splitting, set the parameter random_state equal to 2, this will reproduce similar split during grading.**\n",
        "\n",
        "**For each of the case report MAE, RMSE and R<sup>2</sup>. Which method works better ?(1.50 points)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uLbUAm3tijVQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hvVPd1PijVQ",
        "outputId": "9027f823-ed14-4ad4-cc43-f4a2ecbc5d38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Condition1_RRAe            0\n",
              "SaleCondition_Partial      0\n",
              "MasVnrArea                 8\n",
              "GarageYrBlt               81\n",
              "LotFrontage              259\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# NaN values after preprocessing\n",
        "newhot.isna().sum().sort_values().tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw6jCRXtijVR",
        "outputId": "60507fd5-2d45-4696-ec9f-3883fede0a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   MSSubClass  LotFrontage  ...  SaleCondition_Normal  SaleCondition_Partial\n",
            "0    4.094345     4.174387  ...                   1.0                    0.0\n",
            "1    2.995732     4.382027  ...                   1.0                    0.0\n",
            "2    4.094345     4.219508  ...                   1.0                    0.0\n",
            "3    4.248495     4.094345  ...                   0.0                    0.0\n",
            "4    4.094345     4.430817  ...                   1.0                    0.0\n",
            "5    3.912023     4.442651  ...                   1.0                    0.0\n",
            "6    2.995732     4.317488  ...                   1.0                    0.0\n",
            "7    4.094345     4.191049  ...                   1.0                    0.0\n",
            "8    3.912023     3.931826  ...                   0.0                    0.0\n",
            "9    5.247024     3.912023  ...                   1.0                    0.0\n",
            "\n",
            "[10 rows x 271 columns]\n",
            "   MSSubClass  LotFrontage  ...  SaleCondition_Normal  SaleCondition_Partial\n",
            "0    4.094345     4.174387  ...                   1.0                    0.0\n",
            "1    2.995732     4.382027  ...                   1.0                    0.0\n",
            "2    4.094345     4.219508  ...                   1.0                    0.0\n",
            "3    4.248495     4.094345  ...                   0.0                    0.0\n",
            "4    4.094345     4.430817  ...                   1.0                    0.0\n",
            "5    3.912023     4.442651  ...                   1.0                    0.0\n",
            "6    2.995732     4.317488  ...                   1.0                    0.0\n",
            "7    4.094345     4.234107  ...                   1.0                    0.0\n",
            "8    3.912023     3.931826  ...                   0.0                    0.0\n",
            "9    5.247024     3.912023  ...                   1.0                    0.0\n",
            "\n",
            "[10 rows x 271 columns]\n",
            "   MSSubClass  LotFrontage  ...  SaleCondition_Normal  SaleCondition_Partial\n",
            "0    4.094345     4.174387  ...                   1.0                    0.0\n",
            "1    2.995732     4.382027  ...                   1.0                    0.0\n",
            "2    4.094345     4.219508  ...                   1.0                    0.0\n",
            "3    4.248495     4.094345  ...                   0.0                    0.0\n",
            "4    4.094345     4.430817  ...                   1.0                    0.0\n",
            "5    3.912023     4.442651  ...                   1.0                    0.0\n",
            "6    2.995732     4.317488  ...                   1.0                    0.0\n",
            "7    4.094345     4.094345  ...                   1.0                    0.0\n",
            "8    3.912023     3.931826  ...                   0.0                    0.0\n",
            "9    5.247024     3.912023  ...                   1.0                    0.0\n",
            "\n",
            "[10 rows x 271 columns]\n",
            "    MSSubClass  LotFrontage  ...  SaleCondition_Normal  SaleCondition_Partial\n",
            "0     4.094345     4.174387  ...                   1.0                    0.0\n",
            "1     2.995732     4.382027  ...                   1.0                    0.0\n",
            "2     4.094345     4.219508  ...                   1.0                    0.0\n",
            "3     4.248495     4.094345  ...                   0.0                    0.0\n",
            "4     4.094345     4.430817  ...                   1.0                    0.0\n",
            "5     3.912023     4.442651  ...                   1.0                    0.0\n",
            "6     2.995732     4.317488  ...                   1.0                    0.0\n",
            "8     3.912023     3.931826  ...                   0.0                    0.0\n",
            "9     5.247024     3.912023  ...                   1.0                    0.0\n",
            "10    2.995732     4.248495  ...                   1.0                    0.0\n",
            "\n",
            "[10 rows x 271 columns]\n"
          ]
        }
      ],
      "source": [
        "#all imputations done and assinged different named datas.\n",
        "mean_data = newhot.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
        "\n",
        "median_data = newhot.apply(lambda x: x.fillna(x.median()),axis=0)\n",
        "\n",
        "mode_data = newhot.copy()\n",
        "\n",
        "# only three columns have missing values. That is why we did it like below\n",
        "\n",
        "\n",
        "mode_data['LotFrontage'] = mode_data['LotFrontage'].fillna(mode_data['LotFrontage'].mode()[0])\n",
        "mode_data['MasVnrArea'] = mode_data['MasVnrArea'].fillna(mode_data['MasVnrArea'].mode()[0])\n",
        "mode_data['GarageYrBlt'] = mode_data['GarageYrBlt'].fillna(mode_data['GarageYrBlt'].mode()[0])\n",
        "\n",
        "drop_data = newhot.dropna()\n",
        "\n",
        "print(mean_data.head(10))\n",
        "\n",
        "print(median_data.head(10))\n",
        "\n",
        "print(mode_data.head(10))\n",
        "\n",
        "print(drop_data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vajXqJPyijVR",
        "outputId": "bdac8b4a-c372-4b56-f7ae-b971195f4f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   SalePrice  1stFlrSF  GrLivArea  ...  MasVnrArea  GarageYrBlt  LotFrontage\n",
            "0  12.247694  6.752270   7.444249  ...    5.278115     7.602401     4.174387\n",
            "1  12.109011  7.140453   7.140453  ...    0.000000     7.588830     4.382027\n",
            "2  12.317167  6.824374   7.487734  ...    5.087596     7.601402     4.219508\n",
            "3  11.849398  6.867974   7.448334  ...    0.000000     7.599902     4.094345\n",
            "4  12.429216  7.043160   7.695303  ...    5.857933     7.600902     4.430817\n",
            "5  11.870600  6.679599   7.216709  ...    0.000000     7.597396     4.442651\n",
            "6  12.634603  7.434848   7.434848  ...    5.225747     7.602900     4.317488\n",
            "7  12.206073  7.009409   7.644919  ...    5.480639     7.587311     4.191049\n",
            "8  11.774520  6.929517   7.480992  ...    0.000000     7.565793     3.931826\n",
            "9  11.678440  6.981935   6.981935  ...    0.000000     7.569928     3.912023\n",
            "\n",
            "[10 rows x 7 columns]\n",
            "   SalePrice  1stFlrSF  GrLivArea  ...  MasVnrArea  GarageYrBlt  LotFrontage\n",
            "0  12.247694  6.752270   7.444249  ...    5.278115     7.602401     4.174387\n",
            "1  12.109011  7.140453   7.140453  ...    0.000000     7.588830     4.382027\n",
            "2  12.317167  6.824374   7.487734  ...    5.087596     7.601402     4.219508\n",
            "3  11.849398  6.867974   7.448334  ...    0.000000     7.599902     4.094345\n",
            "4  12.429216  7.043160   7.695303  ...    5.857933     7.600902     4.430817\n",
            "5  11.870600  6.679599   7.216709  ...    0.000000     7.597396     4.442651\n",
            "6  12.634603  7.434848   7.434848  ...    5.225747     7.602900     4.317488\n",
            "7  12.206073  7.009409   7.644919  ...    5.480639     7.587311     4.234107\n",
            "8  11.774520  6.929517   7.480992  ...    0.000000     7.565793     3.931826\n",
            "9  11.678440  6.981935   6.981935  ...    0.000000     7.569928     3.912023\n",
            "\n",
            "[10 rows x 7 columns]\n",
            "   SalePrice  1stFlrSF  GrLivArea  ...  MasVnrArea  GarageYrBlt  LotFrontage\n",
            "0  12.247694  6.752270   7.444249  ...    5.278115     7.602401     4.174387\n",
            "1  12.109011  7.140453   7.140453  ...    0.000000     7.588830     4.382027\n",
            "2  12.317167  6.824374   7.487734  ...    5.087596     7.601402     4.219508\n",
            "3  11.849398  6.867974   7.448334  ...    0.000000     7.599902     4.094345\n",
            "4  12.429216  7.043160   7.695303  ...    5.857933     7.600902     4.430817\n",
            "5  11.870600  6.679599   7.216709  ...    0.000000     7.597396     4.442651\n",
            "6  12.634603  7.434848   7.434848  ...    5.225747     7.602900     4.317488\n",
            "7  12.206073  7.009409   7.644919  ...    5.480639     7.587311     4.094345\n",
            "8  11.774520  6.929517   7.480992  ...    0.000000     7.565793     3.931826\n",
            "9  11.678440  6.981935   6.981935  ...    0.000000     7.569928     3.912023\n",
            "\n",
            "[10 rows x 7 columns]\n",
            "    SalePrice  1stFlrSF  GrLivArea  ...  MasVnrArea  GarageYrBlt  LotFrontage\n",
            "0   12.247694  6.752270   7.444249  ...    5.278115     7.602401     4.174387\n",
            "1   12.109011  7.140453   7.140453  ...    0.000000     7.588830     4.382027\n",
            "2   12.317167  6.824374   7.487734  ...    5.087596     7.601402     4.219508\n",
            "3   11.849398  6.867974   7.448334  ...    0.000000     7.599902     4.094345\n",
            "4   12.429216  7.043160   7.695303  ...    5.857933     7.600902     4.430817\n",
            "5   11.870600  6.679599   7.216709  ...    0.000000     7.597396     4.442651\n",
            "6   12.634603  7.434848   7.434848  ...    5.225747     7.602900     4.317488\n",
            "8   11.774520  6.929517   7.480992  ...    0.000000     7.565793     3.931826\n",
            "9   11.678440  6.981935   6.981935  ...    0.000000     7.569928     3.912023\n",
            "10  11.771436  6.946976   6.946976  ...    0.000000     7.583248     4.248495\n",
            "\n",
            "[10 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "#Data formalized \n",
        "new_features_mean = mean_data[[\"SalePrice\", \"1stFlrSF\", \"GrLivArea\", \"OverallQual\", \"MasVnrArea\", \"GarageYrBlt\", \"LotFrontage\"]]\n",
        "\n",
        "print(new_features_mean.head(10))\n",
        "\n",
        "new_features_median = median_data[[\"SalePrice\", \"1stFlrSF\", \"GrLivArea\", \"OverallQual\", \"MasVnrArea\", \"GarageYrBlt\", \"LotFrontage\"]]\n",
        "\n",
        "print(new_features_median.head(10))\n",
        "\n",
        "new_features_mode = mode_data[[\"SalePrice\", \"1stFlrSF\", \"GrLivArea\", \"OverallQual\", \"MasVnrArea\", \"GarageYrBlt\", \"LotFrontage\"]]\n",
        "\n",
        "print(new_features_mode.head(10))\n",
        "\n",
        "new_features_drop = drop_data[[\"SalePrice\", \"1stFlrSF\", \"GrLivArea\", \"OverallQual\", \"MasVnrArea\", \"GarageYrBlt\", \"LotFrontage\"]]\n",
        "\n",
        "print(new_features_drop.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7NSLLc0GijVS"
      },
      "outputs": [],
      "source": [
        "X_train_mean, X_test_mean, y_train_mean, y_test_mean = train_test_split(new_features_mean.loc[:, new_features_mean.columns != 'SalePrice']\n",
        ", new_features_mean['SalePrice'],\n",
        "                                     train_size = 0.8, \n",
        "                                     test_size = 0.2, \n",
        "                                     random_state = 2)\n",
        "\n",
        "X_train_median, X_test_median, y_train_median, y_test_median = train_test_split(new_features_median.loc[:, new_features_median.columns != 'SalePrice']\n",
        ", new_features_median['SalePrice'],\n",
        "                                     train_size = 0.8, \n",
        "                                     test_size = 0.2, \n",
        "                                     random_state = 2)\n",
        "X_train_mode, X_test_mode, y_train_mode, y_test_mode = train_test_split(new_features_mode.loc[:, new_features_mode.columns != 'SalePrice']\n",
        ", new_features_mode['SalePrice'],\n",
        "                                     train_size = 0.8, \n",
        "                                     test_size = 0.2, \n",
        "                                     random_state = 2)\n",
        "X_train_drop, X_test_drop, y_train_drop, y_test_drop = train_test_split(new_features_drop.loc[:, new_features_drop.columns != 'SalePrice']\n",
        ", new_features_drop['SalePrice'],\n",
        "                                     train_size = 0.8, \n",
        "                                     test_size = 0.2, \n",
        "                                     random_state = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6Vb6mEvuijVT"
      },
      "outputs": [],
      "source": [
        "lr_model_mean = LinearRegression()\n",
        "lr_model_median = LinearRegression()\n",
        "lr_model_mode = LinearRegression()\n",
        "lr_model_drop = LinearRegression()\n",
        "\n",
        "lr_model_mean.fit(X_train_mean, y_train_mean) \n",
        "lr_model_median.fit(X_train_median, y_train_median)\n",
        "lr_model_mode.fit(X_train_mode, y_train_mode)\n",
        "lr_model_drop.fit(X_train_drop, y_train_drop)\n",
        "\n",
        "lr_pred_mean = lr_model_mean.predict(X_test_mean) \n",
        "lr_pred_median = lr_model_median.predict(X_test_median) \n",
        "lr_pred_mode = lr_model_mode.predict(X_test_mode) \n",
        "lr_pred_drop = lr_model_drop.predict(X_test_drop) \n",
        "\n",
        "lr_pred_tr_mean = lr_model_mean.predict(X_train_mean)\n",
        "lr_pred_tr_median = lr_model_median.predict(X_train_median) \n",
        "lr_pred_tr_mode = lr_model_mode.predict(X_train_mode) \n",
        "lr_pred_tr_drop = lr_model_drop.predict(X_train_drop) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Na3w-YeKijVT"
      },
      "outputs": [],
      "source": [
        "#MAE\n",
        "test_set_mae_mean = mean_absolute_error(y_test_mean, lr_pred_mean)\n",
        "test_set_mae_median = mean_absolute_error(y_test_median, lr_pred_median)\n",
        "test_set_mae_mode = mean_absolute_error(y_test_mode, lr_pred_mode)\n",
        "test_set_mae_drop = mean_absolute_error(y_test_drop, lr_pred_drop)\n",
        "\n",
        "#RMSE\n",
        "test_set_rmse_mean = (np.sqrt(mean_squared_error(y_test_mean, lr_pred_mean)))\n",
        "test_set_rmse_median = (np.sqrt(mean_squared_error(y_test_median, lr_pred_median)))\n",
        "test_set_rmse_mode = (np.sqrt(mean_squared_error(y_test_mode, lr_pred_mode)))\n",
        "test_set_rmse_drop = (np.sqrt(mean_squared_error(y_test_drop, lr_pred_drop)))\n",
        "\n",
        "\n",
        "#R^2\n",
        "test_set_r2_mean = r2_score(y_test_mean, lr_pred_mean)\n",
        "test_set_r2_median = r2_score(y_test_median, lr_pred_median)\n",
        "test_set_r2_mode = r2_score(y_test_mode, lr_pred_mode)\n",
        "test_set_r2_drop = r2_score(y_test_drop, lr_pred_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DCaoOjKWJfJt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5cf40f-f36b-4952-942a-3c984845ce2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: mean imputation  MAE: 0.12639079825193167  RMSE: 0.17444293506405248  R2: 0.8191708282201269\n",
            "Method: median imputation  MAE: 0.1264463328297846  RMSE: 0.17464677701060227  R2: 0.8187479723823553\n",
            "Method: mode imputation  MAE: 0.12780980134765554  RMSE: 0.17747801572735086  R2: 0.8128237027600651\n",
            "Method: dropping missing values  MAE: 0.12484414719138363  RMSE: 0.1903257143274362  R2: 0.7526938566547206\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "methods = ['mean imputation', 'median imputation', 'mode imputation', 'dropping missing values']\n",
        "#Store the result in the following variables\n",
        "MAE = [test_set_mae_mean, test_set_mae_median, test_set_mae_mode, test_set_mae_drop]\n",
        "RMSE = [test_set_rmse_mean, test_set_rmse_median, test_set_rmse_mode, test_set_rmse_drop]\n",
        "R2 = [test_set_r2_mean, test_set_r2_median, test_set_r2_mode, test_set_r2_drop]\n",
        "#TODO\n",
        "\n",
        "\n",
        "#print the metrics\n",
        "i = 0\n",
        "\n",
        "for m in methods:\n",
        "    print(\"Method: \" + m + \"  MAE: \" + str(MAE[i]) + \"  RMSE: \" + str(RMSE[i]) + \"  R2: \" + str(R2[i]))\n",
        "    i+=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "di6eU4kRJfJt"
      },
      "source": [
        "<font color='red'> **Answer:**</font> The best method is mean imputation. Because the value of the mean imputation is lower in MAE and RMSE which implies higher accuracy of a regression model. While, it has the highest value of R square which is considered desirable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4mCTiasJfJu"
      },
      "source": [
        "**Please store the best MAE, RMSE, r2_best score in the following variables. We will use these variable to compare ```1.2.7```**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XD8eDSFNJfJv"
      },
      "outputs": [],
      "source": [
        "mae_best = MAE[3]    #best MAE\n",
        "rmse_best = RMSE[0]  #best RMSE\n",
        "r2_best = R2[0]      #best R2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgkUo5YBJfJv"
      },
      "source": [
        "# 1.2 Principal Component Analysis (PCA) (2 points)\n",
        "Our model performs quite good. But there is always room to make it better and simpler. By simpler, we mean the reducing the dimensionality of the dataset so that we can have a simpler linear regression model. <br> <br>If you noticed after one-hot encoding, we have 270 features (columns) but all these features do not hold the same level of information. For example, the first feature may hold 50% of the information required to make the linear regression acheive the performance we already had; the last, (feature number 270) may contribute to only 0.0000001% to the total output. Hence, adding this last variable (actually there could be more) to our linear regression model (read equation) will only increase the complexity of the model; space, time and computational complexity. Therefore, it is wise and desirable to make the model simpler yet performing the best (better). \n",
        "<br> <br>\n",
        "One such way to reduce the dimensionality of the dataset is known as Pricipal Component Analysis. Using this method, we can find out which features contribute the most in our model, therefore, we can wisely select how many we need. We will perform, PCA in this section of the homework. <br><br>\n",
        "\n",
        "*There is another powerful method for dimensionality reduction, named t-SNE. We will use t-sne in future homework. <br><br>*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm5DmYT3JfJv"
      },
      "source": [
        "**1.2.1. From ```1.1.4``` keep the best method to deal with missing values and apply PCA to reduce the number of features. (0.5point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "7A1TBlTQijVW",
        "outputId": "63e9a36b-233a-4626-aeaa-8be30b672c5d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1b1af9f5-16c4-4776-b577-06c91f4edf15\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.384391</td>\n",
              "      <td>1.289634</td>\n",
              "      <td>-0.864345</td>\n",
              "      <td>-1.646739</td>\n",
              "      <td>-0.103962</td>\n",
              "      <td>0.488609</td>\n",
              "      <td>-0.614230</td>\n",
              "      <td>0.241786</td>\n",
              "      <td>0.008057</td>\n",
              "      <td>-0.827340</td>\n",
              "      <td>-0.115615</td>\n",
              "      <td>1.195470</td>\n",
              "      <td>0.128396</td>\n",
              "      <td>-0.502343</td>\n",
              "      <td>-0.404657</td>\n",
              "      <td>-0.102865</td>\n",
              "      <td>-0.718458</td>\n",
              "      <td>-2.083451</td>\n",
              "      <td>-0.177764</td>\n",
              "      <td>-0.200968</td>\n",
              "      <td>0.886411</td>\n",
              "      <td>0.137776</td>\n",
              "      <td>0.018127</td>\n",
              "      <td>0.335053</td>\n",
              "      <td>-0.050478</td>\n",
              "      <td>0.412983</td>\n",
              "      <td>1.094171</td>\n",
              "      <td>-0.631179</td>\n",
              "      <td>0.052947</td>\n",
              "      <td>1.683419</td>\n",
              "      <td>-0.683325</td>\n",
              "      <td>0.405909</td>\n",
              "      <td>-0.221401</td>\n",
              "      <td>-0.421119</td>\n",
              "      <td>-0.925576</td>\n",
              "      <td>0.506992</td>\n",
              "      <td>-0.237976</td>\n",
              "      <td>0.179859</td>\n",
              "      <td>-0.344741</td>\n",
              "      <td>0.307041</td>\n",
              "      <td>-0.672561</td>\n",
              "      <td>-0.667350</td>\n",
              "      <td>0.052896</td>\n",
              "      <td>-0.180286</td>\n",
              "      <td>-0.500284</td>\n",
              "      <td>0.098063</td>\n",
              "      <td>-0.692106</td>\n",
              "      <td>-0.716246</td>\n",
              "      <td>-0.523252</td>\n",
              "      <td>-0.214149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.052386</td>\n",
              "      <td>-0.202517</td>\n",
              "      <td>-0.143891</td>\n",
              "      <td>1.526732</td>\n",
              "      <td>-0.180184</td>\n",
              "      <td>-1.358143</td>\n",
              "      <td>-0.503437</td>\n",
              "      <td>-0.498725</td>\n",
              "      <td>-0.305899</td>\n",
              "      <td>-0.247624</td>\n",
              "      <td>-0.067949</td>\n",
              "      <td>-0.295372</td>\n",
              "      <td>-0.506955</td>\n",
              "      <td>-0.282839</td>\n",
              "      <td>0.171038</td>\n",
              "      <td>0.334294</td>\n",
              "      <td>-0.092889</td>\n",
              "      <td>-0.386872</td>\n",
              "      <td>-0.817288</td>\n",
              "      <td>-0.482044</td>\n",
              "      <td>-0.573193</td>\n",
              "      <td>-0.795182</td>\n",
              "      <td>0.218584</td>\n",
              "      <td>2.564547</td>\n",
              "      <td>-0.821820</td>\n",
              "      <td>-1.229606</td>\n",
              "      <td>0.258761</td>\n",
              "      <td>-1.231773</td>\n",
              "      <td>0.310180</td>\n",
              "      <td>0.338994</td>\n",
              "      <td>1.729611</td>\n",
              "      <td>-0.123443</td>\n",
              "      <td>-2.159998</td>\n",
              "      <td>0.171284</td>\n",
              "      <td>1.620024</td>\n",
              "      <td>-0.686380</td>\n",
              "      <td>0.301623</td>\n",
              "      <td>-1.970346</td>\n",
              "      <td>1.087894</td>\n",
              "      <td>-1.511817</td>\n",
              "      <td>1.102819</td>\n",
              "      <td>2.151801</td>\n",
              "      <td>0.358399</td>\n",
              "      <td>0.417100</td>\n",
              "      <td>0.718744</td>\n",
              "      <td>0.205237</td>\n",
              "      <td>-1.096119</td>\n",
              "      <td>0.441646</td>\n",
              "      <td>1.203587</td>\n",
              "      <td>2.191900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.467462</td>\n",
              "      <td>1.265674</td>\n",
              "      <td>-0.680199</td>\n",
              "      <td>-1.612344</td>\n",
              "      <td>-0.196707</td>\n",
              "      <td>0.163251</td>\n",
              "      <td>-0.325050</td>\n",
              "      <td>-0.018430</td>\n",
              "      <td>-0.039948</td>\n",
              "      <td>-0.857667</td>\n",
              "      <td>-0.057049</td>\n",
              "      <td>1.432197</td>\n",
              "      <td>-0.676535</td>\n",
              "      <td>-0.495145</td>\n",
              "      <td>-0.234247</td>\n",
              "      <td>0.684294</td>\n",
              "      <td>-0.804773</td>\n",
              "      <td>-0.416642</td>\n",
              "      <td>0.876600</td>\n",
              "      <td>-0.012140</td>\n",
              "      <td>0.919226</td>\n",
              "      <td>-0.781508</td>\n",
              "      <td>-0.862729</td>\n",
              "      <td>0.295888</td>\n",
              "      <td>0.711684</td>\n",
              "      <td>1.080278</td>\n",
              "      <td>0.141372</td>\n",
              "      <td>-0.380425</td>\n",
              "      <td>0.093292</td>\n",
              "      <td>-0.364399</td>\n",
              "      <td>0.267001</td>\n",
              "      <td>-0.368894</td>\n",
              "      <td>0.532380</td>\n",
              "      <td>0.984550</td>\n",
              "      <td>0.792891</td>\n",
              "      <td>-0.036703</td>\n",
              "      <td>-0.403494</td>\n",
              "      <td>0.156194</td>\n",
              "      <td>-0.765461</td>\n",
              "      <td>-0.919962</td>\n",
              "      <td>-0.600246</td>\n",
              "      <td>0.073408</td>\n",
              "      <td>0.780295</td>\n",
              "      <td>-0.775194</td>\n",
              "      <td>-0.724812</td>\n",
              "      <td>0.080861</td>\n",
              "      <td>1.043674</td>\n",
              "      <td>-1.757657</td>\n",
              "      <td>0.290584</td>\n",
              "      <td>0.713179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.935716</td>\n",
              "      <td>-0.243310</td>\n",
              "      <td>-1.385132</td>\n",
              "      <td>-0.660956</td>\n",
              "      <td>-0.915597</td>\n",
              "      <td>-0.712230</td>\n",
              "      <td>1.084537</td>\n",
              "      <td>2.262823</td>\n",
              "      <td>-1.294321</td>\n",
              "      <td>0.157544</td>\n",
              "      <td>-0.331799</td>\n",
              "      <td>-0.388772</td>\n",
              "      <td>-0.636812</td>\n",
              "      <td>-0.326469</td>\n",
              "      <td>-0.044065</td>\n",
              "      <td>0.559350</td>\n",
              "      <td>-0.326182</td>\n",
              "      <td>1.161687</td>\n",
              "      <td>1.676981</td>\n",
              "      <td>0.510002</td>\n",
              "      <td>-0.419043</td>\n",
              "      <td>0.087987</td>\n",
              "      <td>1.475475</td>\n",
              "      <td>-0.608036</td>\n",
              "      <td>0.657186</td>\n",
              "      <td>-0.698562</td>\n",
              "      <td>-0.723473</td>\n",
              "      <td>1.125317</td>\n",
              "      <td>-0.500010</td>\n",
              "      <td>1.264968</td>\n",
              "      <td>0.733060</td>\n",
              "      <td>2.476015</td>\n",
              "      <td>-0.956398</td>\n",
              "      <td>-1.426472</td>\n",
              "      <td>-0.592617</td>\n",
              "      <td>0.993182</td>\n",
              "      <td>-0.667899</td>\n",
              "      <td>2.357077</td>\n",
              "      <td>0.146999</td>\n",
              "      <td>-0.450194</td>\n",
              "      <td>-2.113885</td>\n",
              "      <td>-1.971793</td>\n",
              "      <td>1.146515</td>\n",
              "      <td>-0.068529</td>\n",
              "      <td>0.602143</td>\n",
              "      <td>-0.404240</td>\n",
              "      <td>0.381714</td>\n",
              "      <td>-1.312589</td>\n",
              "      <td>1.322877</td>\n",
              "      <td>-0.338607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.363253</td>\n",
              "      <td>2.216208</td>\n",
              "      <td>-0.269723</td>\n",
              "      <td>-0.038592</td>\n",
              "      <td>0.102272</td>\n",
              "      <td>-0.190004</td>\n",
              "      <td>-0.244509</td>\n",
              "      <td>0.340837</td>\n",
              "      <td>-0.031446</td>\n",
              "      <td>-0.300207</td>\n",
              "      <td>-0.042084</td>\n",
              "      <td>0.537942</td>\n",
              "      <td>-0.731172</td>\n",
              "      <td>-0.336048</td>\n",
              "      <td>-0.304630</td>\n",
              "      <td>0.641272</td>\n",
              "      <td>-0.716016</td>\n",
              "      <td>-0.218101</td>\n",
              "      <td>1.236486</td>\n",
              "      <td>0.476106</td>\n",
              "      <td>0.826737</td>\n",
              "      <td>-0.599177</td>\n",
              "      <td>0.238207</td>\n",
              "      <td>0.807657</td>\n",
              "      <td>0.608471</td>\n",
              "      <td>0.736850</td>\n",
              "      <td>0.240333</td>\n",
              "      <td>-0.490236</td>\n",
              "      <td>-0.198693</td>\n",
              "      <td>-1.091997</td>\n",
              "      <td>0.366789</td>\n",
              "      <td>-0.326082</td>\n",
              "      <td>0.949539</td>\n",
              "      <td>1.034783</td>\n",
              "      <td>1.004150</td>\n",
              "      <td>-0.133873</td>\n",
              "      <td>-0.037154</td>\n",
              "      <td>-0.361148</td>\n",
              "      <td>-0.334018</td>\n",
              "      <td>-0.132748</td>\n",
              "      <td>-0.796847</td>\n",
              "      <td>-0.102527</td>\n",
              "      <td>0.452686</td>\n",
              "      <td>-0.387516</td>\n",
              "      <td>-1.069972</td>\n",
              "      <td>0.180146</td>\n",
              "      <td>-0.464709</td>\n",
              "      <td>0.061213</td>\n",
              "      <td>0.099367</td>\n",
              "      <td>-0.479968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>1.677214</td>\n",
              "      <td>-0.362049</td>\n",
              "      <td>0.284345</td>\n",
              "      <td>-0.474436</td>\n",
              "      <td>-0.946588</td>\n",
              "      <td>0.562750</td>\n",
              "      <td>-0.053302</td>\n",
              "      <td>-0.707427</td>\n",
              "      <td>-0.120276</td>\n",
              "      <td>-0.782825</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.399031</td>\n",
              "      <td>-0.291884</td>\n",
              "      <td>-0.696724</td>\n",
              "      <td>0.764887</td>\n",
              "      <td>-0.001039</td>\n",
              "      <td>-0.212387</td>\n",
              "      <td>-0.958895</td>\n",
              "      <td>-1.721383</td>\n",
              "      <td>0.843597</td>\n",
              "      <td>0.250382</td>\n",
              "      <td>0.872923</td>\n",
              "      <td>-1.377778</td>\n",
              "      <td>0.937090</td>\n",
              "      <td>0.247190</td>\n",
              "      <td>-0.022544</td>\n",
              "      <td>0.146429</td>\n",
              "      <td>-0.997715</td>\n",
              "      <td>-0.138288</td>\n",
              "      <td>-0.159719</td>\n",
              "      <td>1.753636</td>\n",
              "      <td>0.223455</td>\n",
              "      <td>0.454245</td>\n",
              "      <td>0.534009</td>\n",
              "      <td>0.595595</td>\n",
              "      <td>-0.537579</td>\n",
              "      <td>0.891350</td>\n",
              "      <td>0.334806</td>\n",
              "      <td>-0.492832</td>\n",
              "      <td>-0.791573</td>\n",
              "      <td>0.420443</td>\n",
              "      <td>-0.118245</td>\n",
              "      <td>0.653390</td>\n",
              "      <td>-0.656438</td>\n",
              "      <td>0.478256</td>\n",
              "      <td>-0.073715</td>\n",
              "      <td>0.332341</td>\n",
              "      <td>0.025119</td>\n",
              "      <td>0.151360</td>\n",
              "      <td>-0.042653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>-1.413786</td>\n",
              "      <td>0.542146</td>\n",
              "      <td>0.088988</td>\n",
              "      <td>0.987490</td>\n",
              "      <td>1.432611</td>\n",
              "      <td>-0.038441</td>\n",
              "      <td>1.958629</td>\n",
              "      <td>-0.912787</td>\n",
              "      <td>0.900321</td>\n",
              "      <td>-0.765213</td>\n",
              "      <td>0.074554</td>\n",
              "      <td>-0.628842</td>\n",
              "      <td>-0.126205</td>\n",
              "      <td>-0.577542</td>\n",
              "      <td>-0.874084</td>\n",
              "      <td>0.340198</td>\n",
              "      <td>-0.397671</td>\n",
              "      <td>0.020669</td>\n",
              "      <td>-0.800335</td>\n",
              "      <td>0.473179</td>\n",
              "      <td>1.336520</td>\n",
              "      <td>0.035328</td>\n",
              "      <td>-0.656716</td>\n",
              "      <td>-0.613899</td>\n",
              "      <td>-1.583616</td>\n",
              "      <td>-2.876709</td>\n",
              "      <td>1.755369</td>\n",
              "      <td>0.825967</td>\n",
              "      <td>0.695728</td>\n",
              "      <td>2.039977</td>\n",
              "      <td>0.698141</td>\n",
              "      <td>-0.524202</td>\n",
              "      <td>-1.906034</td>\n",
              "      <td>-0.687421</td>\n",
              "      <td>1.139288</td>\n",
              "      <td>-0.682908</td>\n",
              "      <td>-1.576510</td>\n",
              "      <td>0.188850</td>\n",
              "      <td>-0.623981</td>\n",
              "      <td>1.747542</td>\n",
              "      <td>-0.638851</td>\n",
              "      <td>-1.076472</td>\n",
              "      <td>1.345169</td>\n",
              "      <td>1.509063</td>\n",
              "      <td>0.740656</td>\n",
              "      <td>-1.410687</td>\n",
              "      <td>-0.302894</td>\n",
              "      <td>0.761468</td>\n",
              "      <td>-0.044683</td>\n",
              "      <td>-0.045678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>0.878088</td>\n",
              "      <td>0.203556</td>\n",
              "      <td>-1.238033</td>\n",
              "      <td>-0.598125</td>\n",
              "      <td>-1.538404</td>\n",
              "      <td>-0.158491</td>\n",
              "      <td>-0.114429</td>\n",
              "      <td>-0.217341</td>\n",
              "      <td>0.156624</td>\n",
              "      <td>0.526864</td>\n",
              "      <td>6.024051</td>\n",
              "      <td>1.638972</td>\n",
              "      <td>0.612352</td>\n",
              "      <td>-0.143660</td>\n",
              "      <td>-0.402176</td>\n",
              "      <td>-0.534372</td>\n",
              "      <td>0.402839</td>\n",
              "      <td>0.178763</td>\n",
              "      <td>-0.904446</td>\n",
              "      <td>-1.895783</td>\n",
              "      <td>0.233135</td>\n",
              "      <td>-0.656973</td>\n",
              "      <td>0.722369</td>\n",
              "      <td>0.547874</td>\n",
              "      <td>1.239732</td>\n",
              "      <td>0.006217</td>\n",
              "      <td>-1.398623</td>\n",
              "      <td>0.744860</td>\n",
              "      <td>0.807354</td>\n",
              "      <td>1.515810</td>\n",
              "      <td>1.552921</td>\n",
              "      <td>-1.766865</td>\n",
              "      <td>0.752208</td>\n",
              "      <td>-1.161589</td>\n",
              "      <td>-0.968772</td>\n",
              "      <td>0.822071</td>\n",
              "      <td>0.591681</td>\n",
              "      <td>1.847550</td>\n",
              "      <td>-1.024188</td>\n",
              "      <td>1.682722</td>\n",
              "      <td>0.303381</td>\n",
              "      <td>1.078500</td>\n",
              "      <td>-1.517923</td>\n",
              "      <td>-1.810994</td>\n",
              "      <td>1.841610</td>\n",
              "      <td>0.542115</td>\n",
              "      <td>-0.570360</td>\n",
              "      <td>-0.910523</td>\n",
              "      <td>0.321621</td>\n",
              "      <td>-0.826199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>-0.970733</td>\n",
              "      <td>-0.978909</td>\n",
              "      <td>-0.822043</td>\n",
              "      <td>2.473576</td>\n",
              "      <td>1.807874</td>\n",
              "      <td>1.928759</td>\n",
              "      <td>2.120083</td>\n",
              "      <td>1.752532</td>\n",
              "      <td>-0.599340</td>\n",
              "      <td>0.045980</td>\n",
              "      <td>-0.847863</td>\n",
              "      <td>0.674572</td>\n",
              "      <td>0.039240</td>\n",
              "      <td>0.576874</td>\n",
              "      <td>1.018596</td>\n",
              "      <td>-1.610392</td>\n",
              "      <td>0.442872</td>\n",
              "      <td>-1.365567</td>\n",
              "      <td>-0.750684</td>\n",
              "      <td>-2.169296</td>\n",
              "      <td>-1.318387</td>\n",
              "      <td>-0.543987</td>\n",
              "      <td>-0.166442</td>\n",
              "      <td>0.971219</td>\n",
              "      <td>-0.953068</td>\n",
              "      <td>1.873515</td>\n",
              "      <td>0.100826</td>\n",
              "      <td>-0.609884</td>\n",
              "      <td>0.425806</td>\n",
              "      <td>-1.178810</td>\n",
              "      <td>-2.791497</td>\n",
              "      <td>1.081849</td>\n",
              "      <td>0.108498</td>\n",
              "      <td>-0.100196</td>\n",
              "      <td>-1.491905</td>\n",
              "      <td>-0.566113</td>\n",
              "      <td>-2.375395</td>\n",
              "      <td>0.707174</td>\n",
              "      <td>-0.333802</td>\n",
              "      <td>-0.002957</td>\n",
              "      <td>-1.430432</td>\n",
              "      <td>0.334676</td>\n",
              "      <td>1.909741</td>\n",
              "      <td>-1.927526</td>\n",
              "      <td>1.886455</td>\n",
              "      <td>-0.766865</td>\n",
              "      <td>-0.730038</td>\n",
              "      <td>-0.836324</td>\n",
              "      <td>1.756401</td>\n",
              "      <td>0.942857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>-1.173896</td>\n",
              "      <td>0.223774</td>\n",
              "      <td>-0.334681</td>\n",
              "      <td>2.118082</td>\n",
              "      <td>-0.802496</td>\n",
              "      <td>1.412068</td>\n",
              "      <td>1.572413</td>\n",
              "      <td>-0.072660</td>\n",
              "      <td>1.099326</td>\n",
              "      <td>-0.311743</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>-1.058603</td>\n",
              "      <td>0.028045</td>\n",
              "      <td>-0.799377</td>\n",
              "      <td>-0.547444</td>\n",
              "      <td>-0.103709</td>\n",
              "      <td>-0.017951</td>\n",
              "      <td>-0.654261</td>\n",
              "      <td>-0.979222</td>\n",
              "      <td>-0.450486</td>\n",
              "      <td>-1.071718</td>\n",
              "      <td>0.767822</td>\n",
              "      <td>0.415579</td>\n",
              "      <td>-1.476942</td>\n",
              "      <td>-1.081687</td>\n",
              "      <td>0.087014</td>\n",
              "      <td>-1.850909</td>\n",
              "      <td>-0.816944</td>\n",
              "      <td>0.436271</td>\n",
              "      <td>-0.255263</td>\n",
              "      <td>-1.809768</td>\n",
              "      <td>0.325485</td>\n",
              "      <td>1.074865</td>\n",
              "      <td>-0.872280</td>\n",
              "      <td>-0.341461</td>\n",
              "      <td>1.062473</td>\n",
              "      <td>0.092497</td>\n",
              "      <td>-1.743979</td>\n",
              "      <td>0.416077</td>\n",
              "      <td>2.212110</td>\n",
              "      <td>0.972790</td>\n",
              "      <td>-0.689008</td>\n",
              "      <td>0.649007</td>\n",
              "      <td>0.005855</td>\n",
              "      <td>-2.197388</td>\n",
              "      <td>1.147545</td>\n",
              "      <td>1.723511</td>\n",
              "      <td>0.397883</td>\n",
              "      <td>-1.501909</td>\n",
              "      <td>-1.361447</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 50 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b1af9f5-16c4-4776-b577-06c91f4edf15')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b1af9f5-16c4-4776-b577-06c91f4edf15 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b1af9f5-16c4-4776-b577-06c91f4edf15');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0         1         2   ...        47        48        49\n",
              "0     0.384391  1.289634 -0.864345  ... -0.716246 -0.523252 -0.214149\n",
              "1    -1.052386 -0.202517 -0.143891  ...  0.441646  1.203587  2.191900\n",
              "2     0.467462  1.265674 -0.680199  ... -1.757657  0.290584  0.713179\n",
              "3     0.935716 -0.243310 -1.385132  ... -1.312589  1.322877 -0.338607\n",
              "4     0.363253  2.216208 -0.269723  ...  0.061213  0.099367 -0.479968\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1455  1.677214 -0.362049  0.284345  ...  0.025119  0.151360 -0.042653\n",
              "1456 -1.413786  0.542146  0.088988  ...  0.761468 -0.044683 -0.045678\n",
              "1457  0.878088  0.203556 -1.238033  ... -0.910523  0.321621 -0.826199\n",
              "1458 -0.970733 -0.978909 -0.822043  ... -0.836324  1.756401  0.942857\n",
              "1459 -1.173896  0.223774 -0.334681  ...  0.397883 -1.501909 -1.361447\n",
              "\n",
              "[1460 rows x 50 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X = mean_data.drop('SalePrice', 1)\n",
        "y = mean_data['SalePrice']\n",
        "\n",
        "#The reason of choosing 50components is the sum of the variances are higher than 95%\n",
        "pca = PCA(n_components=50, whiten=True, svd_solver='randomized', random_state=0)\n",
        "\n",
        "#TODO: fit pca\n",
        "principalComponents = pca.fit(X).fit_transform(X)\n",
        "\n",
        "last_pca = pd.DataFrame(principalComponents)\n",
        "\n",
        "last_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-w3klggJfJw"
      },
      "source": [
        "**1.2.2. What percentage of the variance is explained by the first five components? (0.10 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7qvvBQ-ijVX",
        "outputId": "fb2544d7-66b3-4ba5-9dcf-be3fb34b5fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          0\n",
            "0  0.189351\n",
            "1  0.156509\n",
            "2  0.106065\n",
            "3  0.084445\n",
            "4  0.061622\n",
            "0    59.799184\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "explained_variancedf = pd.DataFrame(explained_variance)\n",
        "\n",
        "print(explained_variancedf.head(5))\n",
        "print(explained_variancedf.head(5).sum()*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQEVP0WjJfJw"
      },
      "source": [
        "<font color='red'> **Answer:**</font> Total percentage of first 5 componenets is 59.79%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7ELb09LJfJw"
      },
      "source": [
        "It would be helpful if we could see all of the variance against the number of components, so a plot would give us a better understanding of the situation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3FzebW5JfJ2"
      },
      "source": [
        "**1.2.3. Please plot the result of PCA you built in ```1.2.1```<br>\n",
        "X-axis=Number of Components, Y-axis=Total explained variance and explain the result.(0.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "0rKZ1bw4ijVZ",
        "outputId": "5bb6033e-7fa2-4ae8-a3e2-b00bb86cabeb"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,7))\n",
        "lw=2\n",
        "plt.plot(explained_variance, color = \"k\", lw=lw)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Total explained variance')\n",
        "    \n",
        "plt.xlim(0, 300)\n",
        "plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "\n",
        "plt.axhline(0.9, c='c')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHnlCo-5JfJ3"
      },
      "source": [
        "<font color='red'> **Answer:**</font> From the graph it seems that the first record's varience is less than 0.2. And then the varience drop till around 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_4PB8XRJfJ4"
      },
      "source": [
        "**1.2.5. Again, from ```1.1.4``` keep the best method to deal with missing values and use PCA to reduce the number of features. But you can use only the number of features that are significant in ```1.1.3```, in this case you have to choose an optimum n_component value based on the PCA plot. Otherwise, you can select all of the features and pass the n_components=37. In all cases, keep random_state for PCA equal to 0. (0.20 points)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "T5V5J9lkJfJ4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "outputId": "8ab2d523-05b7-48ed-da53-618aed144d6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dedd6dbe-a71b-4793-8c94-4d866e6eed22\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.384391</td>\n",
              "      <td>1.289634</td>\n",
              "      <td>-0.864345</td>\n",
              "      <td>-1.646739</td>\n",
              "      <td>-0.103962</td>\n",
              "      <td>0.488609</td>\n",
              "      <td>-0.614230</td>\n",
              "      <td>0.241786</td>\n",
              "      <td>0.008059</td>\n",
              "      <td>-0.827343</td>\n",
              "      <td>-0.115610</td>\n",
              "      <td>1.195477</td>\n",
              "      <td>0.128524</td>\n",
              "      <td>-0.501820</td>\n",
              "      <td>-0.405062</td>\n",
              "      <td>-0.100740</td>\n",
              "      <td>-0.715852</td>\n",
              "      <td>-2.083290</td>\n",
              "      <td>-0.181674</td>\n",
              "      <td>-0.203722</td>\n",
              "      <td>0.880103</td>\n",
              "      <td>0.141223</td>\n",
              "      <td>0.030872</td>\n",
              "      <td>0.351847</td>\n",
              "      <td>-0.080884</td>\n",
              "      <td>0.464448</td>\n",
              "      <td>1.080953</td>\n",
              "      <td>-0.553559</td>\n",
              "      <td>0.096023</td>\n",
              "      <td>1.670572</td>\n",
              "      <td>-0.765468</td>\n",
              "      <td>0.259308</td>\n",
              "      <td>-0.324106</td>\n",
              "      <td>-0.363363</td>\n",
              "      <td>-0.922413</td>\n",
              "      <td>0.434827</td>\n",
              "      <td>-0.605125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.052386</td>\n",
              "      <td>-0.202517</td>\n",
              "      <td>-0.143891</td>\n",
              "      <td>1.526732</td>\n",
              "      <td>-0.180184</td>\n",
              "      <td>-1.358143</td>\n",
              "      <td>-0.503437</td>\n",
              "      <td>-0.498724</td>\n",
              "      <td>-0.305901</td>\n",
              "      <td>-0.247615</td>\n",
              "      <td>-0.067946</td>\n",
              "      <td>-0.295453</td>\n",
              "      <td>-0.507336</td>\n",
              "      <td>-0.284750</td>\n",
              "      <td>0.173596</td>\n",
              "      <td>0.329191</td>\n",
              "      <td>-0.102506</td>\n",
              "      <td>-0.391259</td>\n",
              "      <td>-0.819481</td>\n",
              "      <td>-0.442251</td>\n",
              "      <td>-0.569331</td>\n",
              "      <td>-0.800175</td>\n",
              "      <td>0.147158</td>\n",
              "      <td>2.500901</td>\n",
              "      <td>-0.770407</td>\n",
              "      <td>-1.272023</td>\n",
              "      <td>0.272884</td>\n",
              "      <td>-1.376201</td>\n",
              "      <td>0.296724</td>\n",
              "      <td>0.636186</td>\n",
              "      <td>1.943640</td>\n",
              "      <td>-0.285539</td>\n",
              "      <td>-1.767960</td>\n",
              "      <td>0.499920</td>\n",
              "      <td>1.346136</td>\n",
              "      <td>-0.204658</td>\n",
              "      <td>1.692192</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.467462</td>\n",
              "      <td>1.265674</td>\n",
              "      <td>-0.680199</td>\n",
              "      <td>-1.612344</td>\n",
              "      <td>-0.196707</td>\n",
              "      <td>0.163251</td>\n",
              "      <td>-0.325050</td>\n",
              "      <td>-0.018430</td>\n",
              "      <td>-0.039946</td>\n",
              "      <td>-0.857669</td>\n",
              "      <td>-0.057044</td>\n",
              "      <td>1.432210</td>\n",
              "      <td>-0.676553</td>\n",
              "      <td>-0.494956</td>\n",
              "      <td>-0.233985</td>\n",
              "      <td>0.684713</td>\n",
              "      <td>-0.806841</td>\n",
              "      <td>-0.418944</td>\n",
              "      <td>0.874928</td>\n",
              "      <td>-0.014903</td>\n",
              "      <td>0.916270</td>\n",
              "      <td>-0.788957</td>\n",
              "      <td>-0.858884</td>\n",
              "      <td>0.309976</td>\n",
              "      <td>0.712411</td>\n",
              "      <td>1.082802</td>\n",
              "      <td>0.108069</td>\n",
              "      <td>-0.392760</td>\n",
              "      <td>0.063459</td>\n",
              "      <td>-0.376028</td>\n",
              "      <td>0.320094</td>\n",
              "      <td>-0.332700</td>\n",
              "      <td>0.710852</td>\n",
              "      <td>0.998001</td>\n",
              "      <td>0.817748</td>\n",
              "      <td>-0.066204</td>\n",
              "      <td>-0.724629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.935716</td>\n",
              "      <td>-0.243310</td>\n",
              "      <td>-1.385132</td>\n",
              "      <td>-0.660956</td>\n",
              "      <td>-0.915597</td>\n",
              "      <td>-0.712231</td>\n",
              "      <td>1.084537</td>\n",
              "      <td>2.262823</td>\n",
              "      <td>-1.294318</td>\n",
              "      <td>0.157534</td>\n",
              "      <td>-0.331811</td>\n",
              "      <td>-0.388715</td>\n",
              "      <td>-0.636536</td>\n",
              "      <td>-0.324658</td>\n",
              "      <td>-0.045074</td>\n",
              "      <td>0.563016</td>\n",
              "      <td>-0.319168</td>\n",
              "      <td>1.170453</td>\n",
              "      <td>1.669528</td>\n",
              "      <td>0.502436</td>\n",
              "      <td>-0.433294</td>\n",
              "      <td>0.076067</td>\n",
              "      <td>1.502207</td>\n",
              "      <td>-0.549599</td>\n",
              "      <td>0.638383</td>\n",
              "      <td>-0.656022</td>\n",
              "      <td>-0.773451</td>\n",
              "      <td>1.198729</td>\n",
              "      <td>-0.421818</td>\n",
              "      <td>1.219236</td>\n",
              "      <td>0.410826</td>\n",
              "      <td>2.545402</td>\n",
              "      <td>-1.429934</td>\n",
              "      <td>-1.403274</td>\n",
              "      <td>-0.044392</td>\n",
              "      <td>0.951984</td>\n",
              "      <td>-2.033448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.363253</td>\n",
              "      <td>2.216208</td>\n",
              "      <td>-0.269723</td>\n",
              "      <td>-0.038592</td>\n",
              "      <td>0.102272</td>\n",
              "      <td>-0.190004</td>\n",
              "      <td>-0.244509</td>\n",
              "      <td>0.340837</td>\n",
              "      <td>-0.031446</td>\n",
              "      <td>-0.300206</td>\n",
              "      <td>-0.042084</td>\n",
              "      <td>0.537924</td>\n",
              "      <td>-0.731187</td>\n",
              "      <td>-0.336281</td>\n",
              "      <td>-0.304758</td>\n",
              "      <td>0.640713</td>\n",
              "      <td>-0.715456</td>\n",
              "      <td>-0.218289</td>\n",
              "      <td>1.237400</td>\n",
              "      <td>0.489860</td>\n",
              "      <td>0.827995</td>\n",
              "      <td>-0.587255</td>\n",
              "      <td>0.224643</td>\n",
              "      <td>0.782483</td>\n",
              "      <td>0.601165</td>\n",
              "      <td>0.711915</td>\n",
              "      <td>0.269425</td>\n",
              "      <td>-0.477178</td>\n",
              "      <td>-0.155228</td>\n",
              "      <td>-1.065927</td>\n",
              "      <td>0.322730</td>\n",
              "      <td>-0.264333</td>\n",
              "      <td>1.146392</td>\n",
              "      <td>0.917166</td>\n",
              "      <td>0.761524</td>\n",
              "      <td>-0.280994</td>\n",
              "      <td>-0.241782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>1.677214</td>\n",
              "      <td>-0.362049</td>\n",
              "      <td>0.284345</td>\n",
              "      <td>-0.474436</td>\n",
              "      <td>-0.946588</td>\n",
              "      <td>0.562750</td>\n",
              "      <td>-0.053302</td>\n",
              "      <td>-0.707427</td>\n",
              "      <td>-0.120277</td>\n",
              "      <td>-0.782821</td>\n",
              "      <td>0.000598</td>\n",
              "      <td>0.399026</td>\n",
              "      <td>-0.292048</td>\n",
              "      <td>-0.697294</td>\n",
              "      <td>0.764749</td>\n",
              "      <td>-0.003238</td>\n",
              "      <td>-0.214341</td>\n",
              "      <td>-0.960017</td>\n",
              "      <td>-1.715825</td>\n",
              "      <td>0.844373</td>\n",
              "      <td>0.254982</td>\n",
              "      <td>0.879374</td>\n",
              "      <td>-1.394216</td>\n",
              "      <td>0.910829</td>\n",
              "      <td>0.236103</td>\n",
              "      <td>-0.068563</td>\n",
              "      <td>0.154661</td>\n",
              "      <td>-1.014349</td>\n",
              "      <td>-0.180027</td>\n",
              "      <td>-0.129302</td>\n",
              "      <td>1.820730</td>\n",
              "      <td>0.517044</td>\n",
              "      <td>0.576809</td>\n",
              "      <td>0.450878</td>\n",
              "      <td>0.400259</td>\n",
              "      <td>-0.166764</td>\n",
              "      <td>0.401323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>-1.413786</td>\n",
              "      <td>0.542146</td>\n",
              "      <td>0.088988</td>\n",
              "      <td>0.987490</td>\n",
              "      <td>1.432611</td>\n",
              "      <td>-0.038441</td>\n",
              "      <td>1.958629</td>\n",
              "      <td>-0.912787</td>\n",
              "      <td>0.900319</td>\n",
              "      <td>-0.765212</td>\n",
              "      <td>0.074551</td>\n",
              "      <td>-0.628816</td>\n",
              "      <td>-0.126153</td>\n",
              "      <td>-0.577747</td>\n",
              "      <td>-0.874939</td>\n",
              "      <td>0.340176</td>\n",
              "      <td>-0.393081</td>\n",
              "      <td>0.024396</td>\n",
              "      <td>-0.795038</td>\n",
              "      <td>0.467979</td>\n",
              "      <td>1.334159</td>\n",
              "      <td>0.038198</td>\n",
              "      <td>-0.654079</td>\n",
              "      <td>-0.599842</td>\n",
              "      <td>-1.603205</td>\n",
              "      <td>-2.882912</td>\n",
              "      <td>1.817438</td>\n",
              "      <td>0.867933</td>\n",
              "      <td>0.689176</td>\n",
              "      <td>2.093151</td>\n",
              "      <td>0.791276</td>\n",
              "      <td>-0.530748</td>\n",
              "      <td>-1.759044</td>\n",
              "      <td>-0.448392</td>\n",
              "      <td>1.116601</td>\n",
              "      <td>-1.381924</td>\n",
              "      <td>-1.430909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>0.878088</td>\n",
              "      <td>0.203556</td>\n",
              "      <td>-1.238033</td>\n",
              "      <td>-0.598125</td>\n",
              "      <td>-1.538404</td>\n",
              "      <td>-0.158492</td>\n",
              "      <td>-0.114429</td>\n",
              "      <td>-0.217340</td>\n",
              "      <td>0.156630</td>\n",
              "      <td>0.526854</td>\n",
              "      <td>6.024045</td>\n",
              "      <td>1.639016</td>\n",
              "      <td>0.612563</td>\n",
              "      <td>-0.141228</td>\n",
              "      <td>-0.403419</td>\n",
              "      <td>-0.529246</td>\n",
              "      <td>0.405078</td>\n",
              "      <td>0.183418</td>\n",
              "      <td>-0.918145</td>\n",
              "      <td>-1.920632</td>\n",
              "      <td>0.209670</td>\n",
              "      <td>-0.651667</td>\n",
              "      <td>0.757238</td>\n",
              "      <td>0.597693</td>\n",
              "      <td>1.163947</td>\n",
              "      <td>0.079530</td>\n",
              "      <td>-1.502632</td>\n",
              "      <td>0.926827</td>\n",
              "      <td>0.879640</td>\n",
              "      <td>1.239116</td>\n",
              "      <td>1.480855</td>\n",
              "      <td>-1.732173</td>\n",
              "      <td>0.358983</td>\n",
              "      <td>-1.142844</td>\n",
              "      <td>-0.882723</td>\n",
              "      <td>1.378043</td>\n",
              "      <td>-0.338273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>-0.970733</td>\n",
              "      <td>-0.978909</td>\n",
              "      <td>-0.822043</td>\n",
              "      <td>2.473576</td>\n",
              "      <td>1.807874</td>\n",
              "      <td>1.928759</td>\n",
              "      <td>2.120084</td>\n",
              "      <td>1.752533</td>\n",
              "      <td>-0.599338</td>\n",
              "      <td>0.045982</td>\n",
              "      <td>-0.847862</td>\n",
              "      <td>0.674558</td>\n",
              "      <td>0.039060</td>\n",
              "      <td>0.576282</td>\n",
              "      <td>1.019718</td>\n",
              "      <td>-1.612164</td>\n",
              "      <td>0.436282</td>\n",
              "      <td>-1.368963</td>\n",
              "      <td>-0.751459</td>\n",
              "      <td>-2.161656</td>\n",
              "      <td>-1.319058</td>\n",
              "      <td>-0.549653</td>\n",
              "      <td>-0.187213</td>\n",
              "      <td>0.960577</td>\n",
              "      <td>-0.933936</td>\n",
              "      <td>1.866009</td>\n",
              "      <td>0.034614</td>\n",
              "      <td>-0.665785</td>\n",
              "      <td>0.353457</td>\n",
              "      <td>-1.184068</td>\n",
              "      <td>-2.858174</td>\n",
              "      <td>0.967263</td>\n",
              "      <td>-0.219881</td>\n",
              "      <td>0.037606</td>\n",
              "      <td>-1.311690</td>\n",
              "      <td>-0.436655</td>\n",
              "      <td>-2.404068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>-1.173896</td>\n",
              "      <td>0.223774</td>\n",
              "      <td>-0.334681</td>\n",
              "      <td>2.118082</td>\n",
              "      <td>-0.802496</td>\n",
              "      <td>1.412068</td>\n",
              "      <td>1.572414</td>\n",
              "      <td>-0.072660</td>\n",
              "      <td>1.099326</td>\n",
              "      <td>-0.311749</td>\n",
              "      <td>-0.051003</td>\n",
              "      <td>-1.058570</td>\n",
              "      <td>0.028350</td>\n",
              "      <td>-0.798431</td>\n",
              "      <td>-0.548746</td>\n",
              "      <td>-0.100775</td>\n",
              "      <td>-0.007934</td>\n",
              "      <td>-0.649813</td>\n",
              "      <td>-0.973714</td>\n",
              "      <td>-0.463734</td>\n",
              "      <td>-1.072668</td>\n",
              "      <td>0.763687</td>\n",
              "      <td>0.459461</td>\n",
              "      <td>-1.433212</td>\n",
              "      <td>-1.104912</td>\n",
              "      <td>0.109213</td>\n",
              "      <td>-1.838595</td>\n",
              "      <td>-0.757758</td>\n",
              "      <td>0.486332</td>\n",
              "      <td>-0.350455</td>\n",
              "      <td>-1.992297</td>\n",
              "      <td>0.279416</td>\n",
              "      <td>0.886655</td>\n",
              "      <td>-1.110405</td>\n",
              "      <td>0.029063</td>\n",
              "      <td>-0.083011</td>\n",
              "      <td>0.930214</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dedd6dbe-a71b-4793-8c94-4d866e6eed22')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dedd6dbe-a71b-4793-8c94-4d866e6eed22 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dedd6dbe-a71b-4793-8c94-4d866e6eed22');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "            0         1         2   ...        34        35        36\n",
              "0     0.384391  1.289634 -0.864345  ... -0.922413  0.434827 -0.605125\n",
              "1    -1.052386 -0.202517 -0.143891  ...  1.346136 -0.204658  1.692192\n",
              "2     0.467462  1.265674 -0.680199  ...  0.817748 -0.066204 -0.724629\n",
              "3     0.935716 -0.243310 -1.385132  ... -0.044392  0.951984 -2.033448\n",
              "4     0.363253  2.216208 -0.269723  ...  0.761524 -0.280994 -0.241782\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1455  1.677214 -0.362049  0.284345  ...  0.400259 -0.166764  0.401323\n",
              "1456 -1.413786  0.542146  0.088988  ...  1.116601 -1.381924 -1.430909\n",
              "1457  0.878088  0.203556 -1.238033  ... -0.882723  1.378043 -0.338273\n",
              "1458 -0.970733 -0.978909 -0.822043  ... -1.311690 -0.436655 -2.404068\n",
              "1459 -1.173896  0.223774 -0.334681  ...  0.029063 -0.083011  0.930214\n",
              "\n",
              "[1460 rows x 37 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "X_37 = mean_data.drop('SalePrice', 1)\n",
        "y_37 = mean_data['SalePrice']\n",
        "\n",
        "pca_37 = PCA(n_components=37, whiten=True, svd_solver='randomized', random_state=0)\n",
        "\n",
        "#TODO: fit pca\n",
        "principalComponents_37 = pca_37.fit_transform(X_37)\n",
        "\n",
        "last_pca_37 = pd.DataFrame(principalComponents_37)\n",
        "\n",
        "last_pca_37"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ii2fvRuJfJ4"
      },
      "source": [
        "**1.2.6. Use the new components derived from PCA to predict the house pricing. Keep the ratio of test and train set to 20/80 and the random_state equal to 0. Report MAE, RMSE and R<sup>2</sup> (0.60 point)** <br>\n",
        "*Hint: Now your training data is different. Please use pca.transform(X) function to create your new training dataset. But make sure you have the fitted pca from ```1.2.5```*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5zNvtj0ijVa",
        "outputId": "f9cefdac-e242-4e7d-eca3-56b6e3093610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.13334385848671745  RMSE: 0.18767445304991373  R2: 0.7673597284590572\n"
          ]
        }
      ],
      "source": [
        "pca_X = pca_37.transform(X_37)\n",
        "pca_X_df = pd.DataFrame(pca_X)\n",
        "\n",
        "mean_data_reset = mean_data[\"SalePrice\"].reset_index(drop=True)\n",
        "mean_data_reset = pd.DataFrame(mean_data_reset)\n",
        "\n",
        "new_df = pca_X_df.combine_first(mean_data_reset)\n",
        "new_df\n",
        "\n",
        "\n",
        "#methods = ['mean imputation', 'median imputation', 'mode imputation', 'dropping missing values']\n",
        "#MAE = []\n",
        "#RMSE = []\n",
        "#R2 = []\n",
        "    \n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(new_df.loc[:, new_df.columns != 'SalePrice']\n",
        ", new_df['SalePrice'],\n",
        "                                     train_size = 0.8, \n",
        "                                     test_size = 0.2, \n",
        "                                     random_state = 0)\n",
        "\n",
        "regressor = LinearRegression()\n",
        "\n",
        "#TODO: train the regression model\n",
        "\n",
        "#y_predicted_pca = #TODO: predict on test dataset\n",
        "\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train_pca, y_train_pca)\n",
        "lr_pred = regressor.predict(X_test_pca) \n",
        "lr_pred_tr = regressor.predict(X_train_pca) \n",
        "\n",
        "\n",
        "mae_pca = mean_absolute_error(y_test_pca, lr_pred)\n",
        "\n",
        "rmse_pca = (np.sqrt(mean_squared_error(y_test_pca, lr_pred)))\n",
        "\n",
        "r2_pca = r2_score(y_test_pca, lr_pred)\n",
        "    \n",
        "print(\"MAE: \" + str(mae_pca) + \"  RMSE: \" + str(rmse_pca) + \"  R2: \" + str(r2_pca))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJ8DEaLyJfJ4"
      },
      "source": [
        "**1.2.7 The following cell would calculate the difference between pre-PCA and post-PCA. Please explain the situation based on the differences. (0.1 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "aLFLGYHnJfJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4955c2b6-2a7e-4e61-ef70-a80fa41e03e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE difference after PCA:  -0.00849971129533382\n",
            "RMSE difference after PCA:  -0.013231517985861252\n",
            "R2 difference after PCA:  0.05181109976106968\n"
          ]
        }
      ],
      "source": [
        "print(\"MAE difference after PCA: \", mae_best-mae_pca)\n",
        "print(\"RMSE difference after PCA: \", rmse_best-rmse_pca)\n",
        "print(\"R2 difference after PCA: \", r2_best-r2_pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTZCW4GEJfJ5"
      },
      "source": [
        "<font color='red'> **Answer:**</font> After the PCA we can see that the value of MAE and RMSE increased, while R2 value decreased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVj81M_uJfJ5"
      },
      "source": [
        "## 1.3 Overfitting (5 points)\n",
        "\n",
        "Now our model is comparatively better than the earlier models. It is less complex yet performs the almost the same. Let's dive a little deeper into the model now. In this section, we will check if the model is overfitting. The concept of overfitting has already been delivered in the lectures. However, if you are interesed in honing it up, please take a look here or anywhere you understand better: https://datascience.foundation/sciencewhitepaper/underfitting-and-overfitting-in-machine-learning\n",
        "<br>\n",
        "But, unfortunately it is difficult to know if a model is overfitting or underfitting. One way to know more about model's performance is cross-validation. Cross-validation is also used in the hyperparameter searching to find the best performing model in a given scenario.  \n",
        "We have a few techniques to prevent overfitting and we will focus on \n",
        "- 1.3.1 Cross-validation \n",
        "    - K-Fold cross-validation: Most common (we would apply this one to see the performance of the Linear regression model)\n",
        "    - Leave One Out (LOO): Takes each row as the validation set for once, and trains the model on the rest n-1 rows. Thus, it trains n number of models.\n",
        "\n",
        "    - Leave P-Out (LPO): Creates possible splits after leaving p samples out. For n rows, there would be (nCp) possibile train-test splits.\n",
        "    - (For classification problems) Stratified K-Fold: Ensures relative class proportion is preserved in each train and validation fold. Important when the class label is imbalanced (e.g. 95% label: 1; 5% label: 0).\n",
        "    \n",
        "    *The last three techniques will be discussed in detail in the 7th Lecture.* <br><br>\n",
        "    \n",
        "- 1.3.2 Regularization \n",
        "    - L1 (Lasso)    \n",
        "    - L2 (Ridge)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgnrzErbJfJ5"
      },
      "source": [
        "**1.3.0. Now we have to check if the trained regression model in ```1.1.4``` is overfitting. Please use R<sup>2</sup> value on train and test result to determine the overfitting. Please explain the result from the perspective of the dataset and the value(0.2 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-U9AhTxCJfJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc198b4-d331-44f1-a43f-9d7e02598aa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7899919000354128\n",
            "0.8191708282201269\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.r2_score(y_train_mean, lr_pred_tr_mean))\n",
        "print(metrics.r2_score(y_test_mean, lr_pred_mean))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aejhzWfUJfJ6"
      },
      "source": [
        "<font color='red'> **Answer:**</font> In our result it seems that R2test>R2training so, it indicates that our model generalize well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la1JX3IhJfJ6"
      },
      "source": [
        "**1.3.1 Please apply K-fold=10 fold closs validation on the training dataset of ```1.1.4``` Keep random_state=1, shuffle=True, while performing cross validation, make sure that return_train_score=True.(0.5 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7cZsA7alJfJ6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc4fc8ac-f5bd-49c1-a91e-af7dc759853e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
            "R^2: 0.785 (0.053)\n",
            "MSE: -0.034 (0.010)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "lm = LinearRegression()\n",
        "cv = KFold(n_splits = 10, shuffle = True, random_state = 1)\n",
        "scores = cross_validate(lm, X_train_mean, y_train_mean, scoring=['r2', \"neg_mean_squared_error\"], cv=cv, return_train_score=True)\n",
        "#scores\n",
        "# step-1: create a cross-validation scheme\n",
        "\n",
        "# step-2: specify range of hyperparameters to tune\n",
        "hyper_params = [{'n_features_to_select': list(range(1, 7))}]\n",
        "\n",
        "\n",
        "# step-3: perform grid search\n",
        "# 3.1 specify model\n",
        "lm = LinearRegression()\n",
        "lm.fit(X_train_mean, y_train_mean)\n",
        "\n",
        "rfe = RFE(lm)             \n",
        "\n",
        "# 3.2 call GridSearchCV()\n",
        "model_cv = GridSearchCV(estimator = rfe,  # Feature ranking with recursive feature elimination.\n",
        "                        param_grid = hyper_params, \n",
        "                        scoring= 'r2', \n",
        "                        cv = cv, \n",
        "                        verbose = 1,\n",
        "                        return_train_score=True)      \n",
        "\n",
        "# fit the model\n",
        "model_cv.fit(X_train_mean, y_train_mean)\n",
        "\n",
        "#model_cv.best_params_ (gives the n_features_optimal (6))\n",
        "\n",
        "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
        "#cv_results\n",
        "\n",
        "#TODO: create model\n",
        "# final model with cross validation and param tunning\n",
        "n_features_optimal = 6\n",
        "\n",
        "lm = LinearRegression() # create the model\n",
        "\n",
        "rfe = RFE(lm, n_features_to_select=n_features_optimal)  #Feature ranking with recursive feature elimination.  https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html         \n",
        "rfe = rfe.fit(X_train_mean, y_train_mean) # fit the model\n",
        "\n",
        "# predict prices of X_test\n",
        "y_pred = rfe.predict(X_test_mean) # make prediction\n",
        "r2 = sklearn.metrics.r2_score(y_test_mean, y_pred)\n",
        "mse = metrics.mean_squared_error(y_test_mean, y_pred) # evaluate the model\n",
        "\n",
        "#TODO: evaluate model using R^2, and MSE as evaluation metrics\n",
        "#While setting MSE metrics, make sure you pass the right keyword \n",
        "\n",
        "# report performance\n",
        "print('R^2: %.3f (%.3f)' % (mean(scores['test_r2']), std(scores['test_r2'])))\n",
        "print('MSE: %.3f (%.3f)' % (mean(scores['test_neg_mean_squared_error']), std(scores['test_neg_mean_squared_error'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TALQSL6JJfJ6"
      },
      "source": [
        "**1.3.1.2. Please plot the training and test R<sup>2</sup> value where X-axis=number of folds, Y-axis=R<sup>2</sup> value. Explain the plot, if the model shows overfitting or not.(0.3 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "h6AxRDbsJfJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "8a021a3e-146a-4331-8d8b-a85ff14c92ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fecf97d6b10>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=(16,6))\n",
        "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n",
        "plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n",
        "plt.xlabel('number of features')\n",
        "plt.ylabel('r-squared')\n",
        "plt.title(f\"Optimal Number of Features: {model_cv.best_params_}\")\n",
        "plt.legend(['test score', 'train score'], loc='upper left')\n",
        "#TODO: plot the trendlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq62bO4LJfJ6"
      },
      "source": [
        "<font color='red'> **Answer:**</font> In the exercise 1.3.0 the difference between the R2test and R2training was 0.03 however now the difference is less than 0.01. Also it seems that our train score is slightly big which means that our model has learned the training dataset too well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRU2r7W0JfJ7"
      },
      "source": [
        "**1.3.2  Please apply L1 (Lasso) regularization with variable alpha parameters and report the corresponding alpha value and R<sup>2</sup> value. Use the training split from ```1.1.4``` (1.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "JBLdIUk6JfJ7",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c213329e-8070-4451-ac97-9f57f9165b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0001 : 77.24294310554207\n",
            "0.0002 : 77.19900618445419\n",
            "0.00030000000000000003 : 77.11995901835668\n",
            "0.0004 : 77.0058046636781\n",
            "0.0005 : 76.85653872104197\n",
            "0.0006000000000000001 : 76.67216356483007\n",
            "0.0007 : 76.45267808004469\n",
            "0.0008 : 76.19808384739069\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "\n",
        "\n",
        "cross_val_scores_lasso = [] \n",
        "  \n",
        "# List to maintain the different values of alpha \n",
        "alpha = [] \n",
        "\n",
        "y = new_features_mean['SalePrice']\n",
        "X = new_features_mean.drop('SalePrice', axis = 1) \n",
        "\n",
        "# Loop to for different alpha value \n",
        "for i in range(1, 9): \n",
        "    #TODO: formulate the lasso model where alpha=i * 0.0001\n",
        "    lassoModel = Lasso(alpha = i * 0.001)  # alpha: Regularization strength; must be a positive float.\n",
        "    #TODO: fit the lasso model on whole X,y \n",
        "    lassoModel.fit(X_train_mean, y_train_mean) \n",
        "    #TODO: perform 10 fold cross validation and store the result in score variable\n",
        "    scores = cross_val_score(lassoModel, X, y, cv = 10) \n",
        "    avg_cross_val_score = mean(scores)*100\n",
        "    \n",
        "    cross_val_scores_lasso.append(avg_cross_val_score) \n",
        "    alpha.append(i * 0.0001) \n",
        "  \n",
        "# Loop to print the different values of cross-validation scores \n",
        "for i in range(0, len(alpha)): \n",
        "    print(str(alpha[i])+' : '+str(cross_val_scores_lasso[i])) \n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bfFzmDfJfJ7"
      },
      "source": [
        "**1.3.3. Take the best alpha value from ```1.3.2``` and use it to train a new lasso model and report the  R<sup>2</sup> value on test set. Use the train test split from ```1.1.4```. (0.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "26ebQVQ6JfJ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c0b2c3-1eef-4954-e92e-dd1324d4c331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8204269618460696\n"
          ]
        }
      ],
      "source": [
        "# Building and fitting the Lasso Regression Model \n",
        "from sklearn.model_selection import train_test_split\n",
        "lassoModelBest = Lasso(alpha = 0.0001) # from the above the best alpha value is 0.0001\n",
        "\n",
        "#TODO: Fit the model again \n",
        "lassoModelBest.fit(X_train_mean, y_train_mean)  # 2nd step\n",
        "  \n",
        "# Evaluating the Lasso Regression model \n",
        "print(lassoModelBest.score(X_test_mean, y_test_mean)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNhJ6cwkJfJ7"
      },
      "source": [
        "**1.3.4.  Please apply L2 (Ridge) regularization with variable alpha parameters and report the corresponding alpha value and R<sup>2</sup> value. Use the training split from ```1.1.4``` (1.5 point)**\n",
        "\n",
        "N.B. The $alpha$ here in the ridge regularization is the same as $lambda$ you saw in the lecture. We did not initiate the variable with $lambda$ because $lambda$ is a reserved keyword in python which is used to create small anonymous functions. A $lambda$ function can take any number of arguments, but can only have one expression.\n",
        "You can read more about it here: https://www.w3schools.com/python/ref_keyword_lambda.asp#:~:text=The%20lambda%20keyword%20is%20used,and%20the%20result%20is%20returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "98yYANe8ijVg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso \n",
        "from sklearn.model_selection import train_test_split, cross_val_score \n",
        "from statistics import mean "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "WQXWz0z4JfJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e67532b-8ad2-47b1-b445-cb2b2e5e82c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0001 : 79.10106431742476\n",
            "0.0002 : 79.1013114671388\n",
            "0.00030000000000000003 : 79.10137715002003\n",
            "0.0004 : 79.10126873932174\n",
            "0.0005 : 79.10099330881482\n",
            "0.0006000000000000001 : 79.10055764623613\n",
            "0.0007 : 79.09996826606161\n",
            "0.0008 : 79.0992314216418\n"
          ]
        }
      ],
      "source": [
        "\n",
        "cross_val_scores_ridge = [] \n",
        "  \n",
        "# List to maintain the different values of alpha \n",
        "alpha = [] \n",
        "y = new_features_mean['SalePrice']\n",
        "X = new_features_mean.drop('SalePrice', axis = 1) \n",
        "\n",
        "\n",
        "\n",
        "# Loop to for different alpha value \n",
        "for i in range(1, 9): \n",
        "    #TODO: formulate the ridge model where alpha=i * 0.0001\n",
        "    ridgeModel = Ridge(alpha = i * 0.001)  # alpha: Regularization strength; must be a positive float.\n",
        "\n",
        "\n",
        "    #TODO: fit the ridge model on whole X, y\n",
        "    ridgeModel.fit(X_train_mean, y_train_mean) \n",
        "\n",
        "    #TODO: perform 10 fold cross validation and store the result in score variable\n",
        "    scores = cross_val_score(ridgeModel, X, y, cv = 10) \n",
        "\n",
        "    avg_cross_val_score = mean(scores)*100\n",
        "    \n",
        "    cross_val_scores_ridge.append(avg_cross_val_score) \n",
        "    alpha.append(i * 0.0001) \n",
        "  \n",
        "# Loop to print the different values of cross-validation scores \n",
        "for i in range(0, len(alpha)): \n",
        "    print(str(alpha[i])+' : '+str(cross_val_scores_ridge[i])) \n",
        "    \n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3k6yZXNJfJ8"
      },
      "source": [
        "**1.3.5. Take the best alpha value from ```1.3.4``` and use it to train a new ridge model and report the  R<sup>2</sup> value on test set. Use the train test split from ```1.1.4```. (0.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JB_FtNHeJfJ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0477857e-c6ec-43f3-ce37-4febd85f66f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.819198720119198\n"
          ]
        }
      ],
      "source": [
        "# Building and fitting the Ridge Regression Model \n",
        "from sklearn.model_selection import train_test_split\n",
        "ridgeModelBest = Ridge(alpha = 0.0003) # from the above the best value for alpha is 0.0003\n",
        "\n",
        "#TODO: Fit the model again \n",
        "ridgeModelBest.fit(X_train_mean, y_train_mean)  # 2nd step\n",
        "\n",
        "# Evaluating the ridge Regression model \n",
        "print(ridgeModelBest.score(X_test_mean, y_test_mean)) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HW-2-final .ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}